---
{}
---

Confidence intervals in statistics refer to the probability that a population parameter falls within a set of values under a specific proportion. Analysts typically use confidence intervals that contain 95% or 99% of expected observations. Therefore, if a point estimate of 10.00 is obtained from a statistical model with a 95% confidence interval ranging from 9.50 to 10.50, this means we are 95% confident that the true value falls within this range.

Statisticians and other analysts use confidence intervals to understand the statistical significance of their estimates, inferences, or predictions. If a confidence interval contains zero (or some other null hypothesis), then one cannot satisfactorily claim that the data results generated through testing or experimentation can be attributed to a specific cause rather than chance.

### Key Points

- Confidence intervals show the probability of a parameter falling between two values around the mean.
- Confidence intervals measure the degree of uncertainty or certainty in sampling methods.
- They are also used in hypothesis testing and regression analysis.
- Statisticians often combine p-values with confidence intervals to measure statistical significance.
- They are typically constructed using 95% or 99% confidence levels.

## Understanding Confidence Intervals

Confidence intervals measure the degree of uncertainty or certainty in sampling methods. They can take any number of probability limits, with 95% or 99% confidence levels being the most common. Confidence intervals are established through statistical methods such as t-tests.

Statisticians use confidence intervals to measure uncertainty in population parameter estimates based on samples. For example, researchers randomly select different samples from the same population and calculate confidence intervals for each sample to see how they might reflect the true value of the population variable. The resulting datasets are all different; some intervals contain the true population parameter, while others do not.

A confidence interval is a range of values that lies between upper and lower bounds around a statistical mean and may contain an unknown population parameter. The confidence level represents the probability or percentage certainty that the confidence interval contains the true population parameter when random sampling is repeated multiple times.

In other words, "We are 99% certain (confidence level) that most of these samples (confidence interval) contain the true population parameter."

The biggest misconception about confidence intervals is that they represent the percentage of data from a given sample that falls between the upper and lower bounds. For example, the above 99% confidence interval of 70 to 78 inches might be incorrectly interpreted to mean that 99% of the random sample data falls between these two values.

This interpretation is incorrect, although there is another statistical analysis method that can make such determinations. Performing this analysis requires determining the sample mean and standard deviation and plotting this data on a bell curve.

**Important Note:** Confidence intervals and confidence levels are related but not exactly the same.

## Calculating Confidence Intervals

Suppose a group of researchers is studying the height of high school basketball players. The researchers randomly sample from the population and obtain a mean height of 74 inches.

The mean of 74 inches is a point estimate of the population mean. A point estimate alone has limited use because it doesn't reveal the uncertainty associated with the estimate; therefore, you cannot effectively judge how far this sample mean of 74 inches might be from the population mean. What's missing is the degree of uncertainty in a single sample.

Confidence intervals provide more information than point estimates. By establishing a 95% confidence interval using the sample mean and standard deviation, and assuming a normal distribution with a bell-shaped curve, these researchers determine upper and lower bounds that contain the true mean 95% of the time.

Suppose this interval is between 72 inches and 76 inches. If researchers were to take 100 random samples from the population of high school basketball players, then 95 of those sample means should fall between 72 and 76 inches.

If researchers want a higher level of confidence, they can extend the confidence interval to 99%. This typically results in a wider range because it allows for more sample means. If they determine the 99% confidence interval to be between 70 inches and 78 inches, they can expect 99 out of 100 sample means to fall between these numbers.

Conversely, a 90% confidence level means you can expect 90% of interval estimates to contain the population parameter, and so on.

## What Does a Confidence Interval Reveal?

A confidence interval is a range of values, bounded by upper and lower limits around the mean of a statistic, that may contain an unknown population parameter. The confidence level indicates the probability or percentage certainty that the confidence interval contains the true population parameter when random sampling is repeated multiple times.

## Why Use Confidence Intervals?

Statisticians use confidence intervals to measure the uncertainty of sample variables. For example, researchers randomly select different samples from the same population and calculate confidence intervals for each sample to see how they might reflect the true value of the population variable. The resulting datasets are all different, with some intervals containing the true population parameter and others not containing it.

## What are common misconceptions about confidence intervals?

The biggest misconception about confidence intervals is that they represent the percentage of data from a given sample that falls between the upper and lower bounds. In other words, it's incorrect to assume that a 99% confidence interval means 99% of random sample data falls between these bounds. In reality, it means there is 99% confidence that this range contains the population mean.

## What is a t-test?

Confidence intervals are established using statistical methods, such as t-tests. A t-test is a type of inferential statistics used to determine whether there is a significant difference between the means of two groups, which may be related to certain characteristics. Calculating a t-test requires three key data values, including the difference between the means of each dataset (called the mean difference), the standard deviation of each group, and the number of data values in each group.

## How to Interpret P-values and Confidence Intervals?

The p-value is a statistical measure used to verify the relationship between hypotheses and observed data, measuring the probability of obtaining the observed results when the null hypothesis is true. Generally, a p-value less than 0.05 is considered statistically significant, at which point the null hypothesis should be rejected. This corresponds to some extent to the probability that the null hypothesis value (usually zero) is contained within the 95% confidence interval.

## Conclusion

Confidence intervals enable analysts to understand whether statistical analysis results are genuine or coincidental. When attempting to make inferences or predictions based on sample data, there is inherent uncertainty about whether the analytical results align with the true population being studied. Confidence intervals delineate the range within which the true value is likely to fall.